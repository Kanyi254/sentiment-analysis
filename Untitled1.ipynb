{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7gnsttaG3mmV6U9VTN5g4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kanyi254/sentiment-analysis/blob/Maureen/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SENTIMENT ANALYSIS\n",
        "\n",
        "## 1. INTRODUCTION\n",
        "Sentiment analysis, also known as opinion mining, is the process of analyzing and categorizing opinions expressed in text data to determine the writer's attitude toward a particular subject, product, or service. In the context of business and marketing, this process is vital for understanding customer sentiment from publicly available data, such as social media posts. Twitter, in particular, is a valuable source of real-time user-generated content that can provide deep insights into customer perceptions of brands, products, and services.\n",
        "\n",
        "In this project, we aim to perform sentiment analysis on tweets to classify them as expressing either positive or negative sentiments about different products or companies. This will help businesses monitor customer feedback, improve product offerings, and manage brand perception.\n",
        "\n",
        "### Problem Statement\n",
        "With the growing volume of user-generated content on platforms like Twitter, businesses struggle to keep up with real-time customer feedback. Manually categorizing thousands of tweets is inefficient and time-consuming. Automated sentiment analysis offers an efficient solution for determining whether customers express positive or negative sentiments about a product or service.\n",
        "\n",
        "The challenge is to build a machine learning model that can accurately classify the sentiment of tweets related to various products and companies.\n",
        "\n",
        "### Objective\n",
        "The objective of this project is to develop a machine learning model that can automatically classify the sentiment of tweets as either positive, negative or neutral. The model will be built using appropriate machine learning techniques, evaluated with standard metrics (such as accuracy, precision, recall, and F1 score), and applied to unseen data. The project will also include model explainability through techniques like LIME to ensure that predictions can be interpreted by users.\n",
        "\n",
        "### Data Sources\n",
        "The dataset originates from CrowdFlower via data.world. Contributors evaluated tweets related to various brands and products. Specifically:\n",
        "\n",
        "- Each tweet was labeled as expressing positive, negative, or no emotion toward a brand or product.\n",
        "- If emotion was expressed, contributors specified which brand or product was the target.\n",
        "\n",
        "### Project Workflow\n",
        "1. Data Loading and Understanding\n",
        "2. Data Cleaning\n",
        "3. Exploratory Data Analysis (EDA)\n",
        "4. Data Preprocessing (for NLP tasks)\n",
        "5. Modeling\n",
        "6. Evaluation and Model Explainability\n",
        "\n",
        "## 2. DATA LOADING & DATA UNDERSTANDING\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ul3v4qNirw4O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPRB_6vKqs0Z",
        "outputId": "515e2274-b6ed-4c7f-8aee-6ad770d1aec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (10.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# plotting\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# nltk\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# sklearn\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse import issparse\n",
        "\n",
        "# wordCloud\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# pickle\n",
        "import pickle\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EdG4jvB0vma",
        "outputId": "26141dc3-5a4a-4a4d-90da-d723a1b0e3b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataUnderstanding():\n",
        "    \"\"\"Class that gives the data understanding of a dataset\"\"\"\n",
        "    def __init__(self, data='None'):\n",
        "        \"\"\"Initialisation\"\"\"\n",
        "        self.df = data\n",
        "\n",
        "    def load_data(self, path):\n",
        "        \"\"\"Loading the data\"\"\"\n",
        "        if self.df == 'None':\n",
        "            self.df = pd.read_csv(path, encoding='latin-1')\n",
        "        return self.df\n",
        "\n",
        "    def understanding(self):\n",
        "        # Info\n",
        "        print(\"\"\"INFO\"\"\")\n",
        "        print(\"-\"*4)\n",
        "        self.df.info()\n",
        "\n",
        "        # Shape\n",
        "        print(\"\"\"\\n\\nSHAPE\"\"\")\n",
        "        print(\"-\"*5)\n",
        "        print(f\"Records in dataset are {self.df.shape[0]} with {self.df.shape[1]} columns.\")\n",
        "\n",
        "        # Columns\n",
        "        print(\"\\n\\nCOLUMNS\")\n",
        "        print(\"-\"*6)\n",
        "        print(f\"Columns in the dataset are:\")\n",
        "        for idx in self.df.columns:\n",
        "            print(f\"- {idx}\")\n",
        "\n",
        "        # Unique Values\n",
        "        print(\"\\n\\nUNIQUE VALUES\")\n",
        "        print(\"-\"*12)\n",
        "        for col in self.df.columns:\n",
        "            print(f\"Column *{col}* has {self.df[col].nunique()} unique values\")\n",
        "            if self.df[col].nunique() < 12:\n",
        "                print(f\"Top unique values in the *{col}* include:\")\n",
        "                for idx in self.df[col].value_counts().index:\n",
        "                    print(f\"- {idx}\")\n",
        "            print(\"\")\n",
        "\n",
        "        # Missing or Null Values\n",
        "        print(\"\"\"\\nMISSING VALUES\"\"\")\n",
        "        print(\"-\"*15)\n",
        "        for col in self.df.columns:\n",
        "            print(f\"Column *{col}* has {self.df[col].isnull().sum()} missing values.\")\n",
        "\n",
        "        # Duplicate Values\n",
        "        print(\"\"\"\\n\\nDUPLICATE VALUES\"\"\")\n",
        "        print(\"-\"*16)\n",
        "        print(f\"The dataset has {self.df.duplicated().sum()} duplicated records.\")"
      ],
      "metadata": {
        "id": "PCWpG3ZXDHWi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset\n",
        "\n",
        "data = DataUnderstanding()\n",
        "\n",
        "df = data.load_data(path=\"judge-1377884607_tweet_product_company.csv\")\n",
        "\n",
        "# First five rows of dataset\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "1Ecr1ZT90_P5",
        "outputId": "7b207ef9-0512-4b90-a4f7-40ae04da3122"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'judge-1377884607_tweet_product_company.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a9e6fe6623be>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataUnderstanding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"judge-1377884607_tweet_product_company.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# First five rows of dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-30adb7269643>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m\"\"\"Loading the data\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'None'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'judge-1377884607_tweet_product_company.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#looking into the dataset\n",
        "data.understanding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfPCbuoYDrqq",
        "outputId": "59ec3850-9b5c-4be2-f7c5-7b73cdb589f9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO\n",
            "----\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9093 entries, 0 to 9092\n",
            "Data columns (total 3 columns):\n",
            " #   Column                                              Non-Null Count  Dtype \n",
            "---  ------                                              --------------  ----- \n",
            " 0   tweet_text                                          9092 non-null   object\n",
            " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
            " 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\n",
            "dtypes: object(3)\n",
            "memory usage: 213.2+ KB\n",
            "\n",
            "\n",
            "SHAPE\n",
            "-----\n",
            "Records in dataset are 9093 with 3 columns.\n",
            "\n",
            "\n",
            "COLUMNS\n",
            "------\n",
            "Columns in the dataset are:\n",
            "- tweet_text\n",
            "- emotion_in_tweet_is_directed_at\n",
            "- is_there_an_emotion_directed_at_a_brand_or_product\n",
            "\n",
            "\n",
            "UNIQUE VALUES\n",
            "------------\n",
            "Column *tweet_text* has 9065 unique values\n",
            "\n",
            "Column *emotion_in_tweet_is_directed_at* has 9 unique values\n",
            "Top unique values in the *emotion_in_tweet_is_directed_at* include:\n",
            "- iPad\n",
            "- Apple\n",
            "- iPad or iPhone App\n",
            "- Google\n",
            "- iPhone\n",
            "- Other Google product or service\n",
            "- Android App\n",
            "- Android\n",
            "- Other Apple product or service\n",
            "\n",
            "Column *is_there_an_emotion_directed_at_a_brand_or_product* has 4 unique values\n",
            "Top unique values in the *is_there_an_emotion_directed_at_a_brand_or_product* include:\n",
            "- No emotion toward brand or product\n",
            "- Positive emotion\n",
            "- Negative emotion\n",
            "- I can't tell\n",
            "\n",
            "\n",
            "MISSING VALUES\n",
            "---------------\n",
            "Column *tweet_text* has 1 missing values.\n",
            "Column *emotion_in_tweet_is_directed_at* has 5802 missing values.\n",
            "Column *is_there_an_emotion_directed_at_a_brand_or_product* has 0 missing values.\n",
            "\n",
            "\n",
            "DUPLICATE VALUES\n",
            "----------------\n",
            "The dataset has 22 duplicated records.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Understanding Summary\n",
        "Our dataset consists of 9,093 records and 3 columns.\n",
        "The dataset primarily contains information from tweets, along with the emotions expressed toward specific brands or products.\n",
        "\n",
        "1. Columns in the Dataset\n",
        "- tweet_text: The text of the tweet. This column contains 9,092 non-null values, meaning 1 record has a missing tweet text. Additionally, there are 9,065 unique tweets, indicating 22 duplicate entries.\n",
        "\n",
        "- emotion_in_tweet_is_directed_at: This column specifies the brand or product the emotion in the tweet is directed at. It has 3,291 non-null values, meaning that 5,802 records have missing values in this column. There are 9 unique values, with the most common brands/products being iPad, Apple, Google, iPhone, and other Google/Apple products or services.\n",
        "\n",
        "- is_there_an_emotion_directed_at_a_brand_or_product: This column describes whether the tweet expresses an emotion toward a brand or product. It has 4 unique values:\n",
        "\n",
        "    - \"No emotion toward brand or product\"\n",
        "\n",
        "    - \"Positive emotion\"\n",
        "\n",
        "    - \"Negative emotion\"\n",
        "\n",
        "    - \"I can't tell\" This column contains no missing values.\n",
        "    \n",
        "2. Missing Values\n",
        "- tweet_text: 1 missing value, which needs to be handled.\n",
        "- emotion_in_tweet_is_directed_at: 5,802 missing values. This large number of missing values may require imputation or exclusion, depending on the importance of this column to the analysis.\n",
        "\n",
        "3. Duplicate Values\n",
        "The dataset contains 22 duplicate records, which need to be addressed to ensure data integrity.\n"
      ],
      "metadata": {
        "id": "cIwVNxuxKzRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. DATA CLEANING\n",
        "\n",
        "### Handling missing value\n",
        "- Column (tweet_text) There is 1 missing value in this column. Since this is the key feature for sentiment analysis, we will be dropping the row containing the missing tweet.\n",
        "- Column (emotion_in_tweet_is_directed_at) has 5802 missing values. Since this column represents the product or brand mentioned in the tweet, we will impute missing values with \"Unknown\" since this column is important and dropping it would mean loosing alot of valuable data and insights.\n",
        "\n"
      ],
      "metadata": {
        "id": "eFhlsOmCNoXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing tweet text\n",
        "data_cleaned = data.dropna(subset=['tweet_text'])\n"
      ],
      "metadata": {
        "id": "YXPzyc45PxE7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}