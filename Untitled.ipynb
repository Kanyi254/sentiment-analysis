{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b861a6-a480-464c-9596-087b1ae63093",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS\n",
    "\n",
    "## 1. INTRODUCTION\n",
    "Sentiment analysis, also known as opinion mining, is the process of analyzing and categorizing opinions expressed in text data to determine the writer's attitude toward a particular subject, product, or service. In the context of business and marketing, this process is vital for understanding customer sentiment from publicly available data, such as social media posts. Twitter, in particular, is a valuable source of real-time user-generated content that can provide deep insights into customer perceptions of brands, products, and services.\n",
    "\n",
    "In this project, we aim to perform sentiment analysis on tweets to classify them as expressing either positive or negative sentiments about different products or companies. This will help businesses monitor customer feedback, improve product offerings, and manage brand perception.\n",
    "\n",
    "### Problem Statement\n",
    "With the growing volume of user-generated content on platforms like Twitter, businesses struggle to keep up with real-time customer feedback. Manually categorizing thousands of tweets is inefficient and time-consuming. Automated sentiment analysis offers an efficient solution for determining whether customers express positive or negative sentiments about a product or service.\n",
    "\n",
    "The challenge is to build a machine learning model that can accurately classify the sentiment of tweets related to various products and companies.\n",
    "\n",
    "### Objective\n",
    "The objective of this project is to develop a machine learning model that can automatically classify the sentiment of tweets as either positive, negative or neutral. The model will be built using appropriate machine learning techniques, evaluated with standard metrics (such as accuracy, precision, recall, and F1 score), and applied to unseen data. The project will also include model explainability through techniques like LIME to ensure that predictions can be interpreted by users.\n",
    "\n",
    "### Data Sources\n",
    "The dataset originates from CrowdFlower via data.world. Contributors evaluated tweets related to various brands and products. Specifically:\n",
    "\n",
    "- Each tweet was labeled as expressing positive, negative, or no emotion toward a brand or product.\n",
    "- If emotion was expressed, contributors specified which brand or product was the target.\n",
    "\n",
    "### Project Workflow\n",
    "1. Data Loading and Understanding\n",
    "2. Data Cleaning\n",
    "3. Exploratory Data Analysis (EDA)\n",
    "4. Data Preprocessing (for NLP tasks)\n",
    "5. Modeling\n",
    "6. Evaluation and Model Explainability\n",
    "\n",
    "## 2. DATA LOADING & DATA UNDERSTANDING\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e1b3f4b-ba2b-4d7e-b98b-3be97290c1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\nkatha\\anaconda3\\envs\\learn-env\\lib\\site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\nkatha\\anaconda3\\envs\\learn-env\\lib\\site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\nkatha\\anaconda3\\envs\\learn-env\\lib\\site-packages (from wordcloud) (10.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\nkatha\\anaconda3\\envs\\learn-env\\lib\\site-packages (from wordcloud) (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nkatha\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nkatha\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nkatha\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->wordcloud) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\nkatha\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nkatha\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->wordcloud) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nkatha\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nkatha\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nkatha\\anaconda3\\envs\\learn-env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e577706-ce20-4ba6-ad06-f664e84712ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FreqDist\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\learn-env\\Lib\\site-packages\\nltk\\__init__.py:138\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrammar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjsontags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\learn-env\\Lib\\site-packages\\nltk\\text.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConditionalFreqDist \u001b[38;5;28;01mas\u001b[39;00m CFD\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FreqDist\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyConcatenation, cut_string, tokenwrap\n\u001b[0;32m     33\u001b[0m ConcordanceLine \u001b[38;5;241m=\u001b[39m namedtuple(\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcordanceLine\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     35\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_print\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright_print\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     36\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\learn-env\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:65\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcasual\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TweetTokenizer, casual_tokenize\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdestructive\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NLTKWordTokenizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk.data'"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "# wordCloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# pickle\n",
    "import pickle\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f07b48a-600d-4662-820d-af9c2eec7058",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUnderstanding():\n",
    "    \"\"\"Class that gives the data understanding of a dataset\"\"\"\n",
    "    def __init__(self, data='None'):\n",
    "        \"\"\"Initialisation\"\"\"\n",
    "        self.df = data\n",
    "\n",
    "    def load_data(self, path):\n",
    "        \"\"\"Loading the data\"\"\"\n",
    "        if self.df == 'None':\n",
    "            self.df = pd.read_csv(path, encoding='latin-1')\n",
    "        return self.df\n",
    "\n",
    "    def understanding(self):\n",
    "        # Info\n",
    "        print(\"\"\"INFO\"\"\")\n",
    "        print(\"-\"*4)\n",
    "        self.df.info()\n",
    "\n",
    "        # Shape\n",
    "        print(\"\"\"\\n\\nSHAPE\"\"\")\n",
    "        print(\"-\"*5)\n",
    "        print(f\"Records in dataset are {self.df.shape[0]} with {self.df.shape[1]} columns.\")\n",
    "\n",
    "        # Columns\n",
    "        print(\"\\n\\nCOLUMNS\")\n",
    "        print(\"-\"*6)\n",
    "        print(f\"Columns in the dataset are:\")\n",
    "        for idx in self.df.columns:\n",
    "            print(f\"- {idx}\")\n",
    "\n",
    "        # Unique Values\n",
    "        print(\"\\n\\nUNIQUE VALUES\")\n",
    "        print(\"-\"*12)\n",
    "        for col in self.df.columns:\n",
    "            print(f\"Column *{col}* has {self.df[col].nunique()} unique values\")\n",
    "            if self.df[col].nunique() < 12:\n",
    "                print(f\"Top unique values in the *{col}* include:\")\n",
    "                for idx in self.df[col].value_counts().index:\n",
    "                    print(f\"- {idx}\")\n",
    "            print(\"\")\n",
    "\n",
    "        # Missing or Null Values\n",
    "        print(\"\"\"\\nMISSING VALUES\"\"\")\n",
    "        print(\"-\"*15)\n",
    "        for col in self.df.columns:\n",
    "            print(f\"Column *{col}* has {self.df[col].isnull().sum()} missing values.\")\n",
    "\n",
    "        # Duplicate Values\n",
    "        print(\"\"\"\\n\\nDUPLICATE VALUES\"\"\")\n",
    "        print(\"-\"*16)\n",
    "        print(f\"The dataset has {self.df.duplicated().sum()} duplicated records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "451aecdd-3f60-434f-91dd-cebc5d5e807b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "data = DataUnderstanding()\n",
    "\n",
    "df = data.load_data(path=\"judge-1377884607_tweet_product_company.csv\")\n",
    "\n",
    "# First five rows of dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb5b640d-9f9c-4372-8c1a-19575d2f0f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO\n",
      "----\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n",
      "\n",
      "\n",
      "SHAPE\n",
      "-----\n",
      "Records in dataset are 9093 with 3 columns.\n",
      "\n",
      "\n",
      "COLUMNS\n",
      "------\n",
      "Columns in the dataset are:\n",
      "- tweet_text\n",
      "- emotion_in_tweet_is_directed_at\n",
      "- is_there_an_emotion_directed_at_a_brand_or_product\n",
      "\n",
      "\n",
      "UNIQUE VALUES\n",
      "------------\n",
      "Column *tweet_text* has 9065 unique values\n",
      "\n",
      "Column *emotion_in_tweet_is_directed_at* has 9 unique values\n",
      "Top unique values in the *emotion_in_tweet_is_directed_at* include:\n",
      "- iPad\n",
      "- Apple\n",
      "- iPad or iPhone App\n",
      "- Google\n",
      "- iPhone\n",
      "- Other Google product or service\n",
      "- Android App\n",
      "- Android\n",
      "- Other Apple product or service\n",
      "\n",
      "Column *is_there_an_emotion_directed_at_a_brand_or_product* has 4 unique values\n",
      "Top unique values in the *is_there_an_emotion_directed_at_a_brand_or_product* include:\n",
      "- No emotion toward brand or product\n",
      "- Positive emotion\n",
      "- Negative emotion\n",
      "- I can't tell\n",
      "\n",
      "\n",
      "MISSING VALUES\n",
      "---------------\n",
      "Column *tweet_text* has 1 missing values.\n",
      "Column *emotion_in_tweet_is_directed_at* has 5802 missing values.\n",
      "Column *is_there_an_emotion_directed_at_a_brand_or_product* has 0 missing values.\n",
      "\n",
      "\n",
      "DUPLICATE VALUES\n",
      "----------------\n",
      "The dataset has 22 duplicated records.\n"
     ]
    }
   ],
   "source": [
    "#looking into the dataset\n",
    "data.understanding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd573f6-fa68-4d55-842d-76f5c159bec2",
   "metadata": {},
   "source": [
    "### Data Understanding Summary\n",
    "Our dataset consists of 9,093 records and 3 columns. \n",
    "The dataset primarily contains information from tweets, along with the emotions expressed toward specific brands or products.\n",
    "\n",
    "1. Columns in the Dataset\n",
    "- tweet_text: The text of the tweet. This column contains 9,092 non-null values, meaning 1 record has a missing tweet text. Additionally, there are 9,065 unique tweets, indicating 22 duplicate entries.\n",
    "\n",
    "- emotion_in_tweet_is_directed_at: This column specifies the brand or product the emotion in the tweet is directed at. It has 3,291 non-null values, meaning that 5,802 records have missing values in this column. There are 9 unique values, with the most common brands/products being iPad, Apple, Google, iPhone, and other Google/Apple products or services.\n",
    "\n",
    "- is_there_an_emotion_directed_at_a_brand_or_product: This column describes whether the tweet expresses an emotion toward a brand or product. It has 4 unique values:\n",
    "\n",
    "    - \"No emotion toward brand or product\"\n",
    "\n",
    "    - \"Positive emotion\"\n",
    "\n",
    "    - \"Negative emotion\"\n",
    "\n",
    "    - \"I can't tell\" This column contains no missing values.\n",
    "    \n",
    "2. Missing Values\n",
    "- tweet_text: 1 missing value, which needs to be handled.\n",
    "- emotion_in_tweet_is_directed_at: 5,802 missing values. This large number of missing values may require imputation or exclusion, depending on the importance of this column to the analysis.\n",
    "\n",
    "3. Duplicate Values\n",
    "The dataset contains 22 duplicate records, which need to be addressed to ensure data integrity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0fd3d2-df46-4e78-a36a-1b77ab953dd4",
   "metadata": {},
   "source": [
    "## 3. DATA CLEANING\n",
    "\n",
    "### Handling missing value\n",
    "- Column (tweet_text) There is 1 missing value in this column. Since this is the key feature for sentiment analysis, we will be dropping the row containing the missing tweet.\n",
    "- Column (emotion_in_tweet_is_directed_at) has 5802 missing values. Since this column represents the product or brand mentioned in the tweet, we will impute missing values with \"Unknown\" since this column is important and dropping it would mean loosing alot of valuable data and insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d255aeb0-8f40-4a44-9c80-ce7077e9fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing tweet text\n",
    "data_cleaned = df.dropna(subset=['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5feda85-be09-435b-a918-968a740c8ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nkatha\\AppData\\Local\\Temp\\ipykernel_1916\\1328872757.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_cleaned['emotion_in_tweet_is_directed_at'].fillna('Unknown', inplace=True)\n",
      "C:\\Users\\nkatha\\AppData\\Local\\Temp\\ipykernel_1916\\1328872757.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_cleaned['emotion_in_tweet_is_directed_at'].fillna('Unknown', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values with 'Unknown'\n",
    "data_cleaned['emotion_in_tweet_is_directed_at'].fillna('Unknown', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b2be2-bf54-4b40-b365-f711f904236c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
